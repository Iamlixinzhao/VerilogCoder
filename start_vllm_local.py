#!/usr/bin/env python3
"""
å¯åŠ¨vLLMæœ¬åœ°æœåŠ¡è„šæœ¬
ç”¨äºåœ¨æœ¬åœ°æµ‹è¯•VerilogCoderé¡¹ç›®
"""

import subprocess
import sys
import time
import requests
import json
from pathlib import Path

def check_vllm_installation():
    """æ£€æŸ¥vLLMæ˜¯å¦å·²å®‰è£…"""
    try:
        subprocess.run(["vllm", "--help"], capture_output=True, check=True)
        print("âœ… vLLMå·²å®‰è£…")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ vLLMæœªå®‰è£…")
        print("è¯·å…ˆå®‰è£…vLLM: pip install vllm")
        return False

def start_vllm_service(model_name="microsoft/DialoGPT-medium", port=8000):
    """å¯åŠ¨vLLMæœåŠ¡"""
    print(f"ğŸš€ å¯åŠ¨vLLMæœåŠ¡...")
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   ç«¯å£: {port}")
    print(f"   APIç«¯ç‚¹: http://localhost:{port}/v1")
    
    # æ„å»ºvLLMå¯åŠ¨å‘½ä»¤
    cmd = [
        "vllm", "serve", model_name,
        "--host", "0.0.0.0",
        "--port", str(port),
        "--openai-compatible"
    ]
    
    try:
        print(f"ğŸ“ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print("â³ å¯åŠ¨ä¸­ï¼Œè¯·ç­‰å¾…...")
        
        # å¯åŠ¨vLLMæœåŠ¡
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        print("ğŸ”„ ç­‰å¾…æœåŠ¡å¯åŠ¨...")
        time.sleep(10)
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨æˆåŠŸ
        if check_service_health(port):
            print("âœ… vLLMæœåŠ¡å¯åŠ¨æˆåŠŸï¼")
            print(f"ğŸŒ æœåŠ¡åœ°å€: http://localhost:{port}")
            print(f"ğŸ“š APIæ–‡æ¡£: http://localhost:{port}/docs")
            print("\nğŸ’¡ ç°åœ¨ä½ å¯ä»¥:")
            print("   1. ä¿®æ”¹OAI_CONFIG_LISTä¸ºVLLM_CONFIG_LIST")
            print("   2. è¿è¡ŒVerilogCoderé¡¹ç›®")
            print("   3. æŒ‰Ctrl+Cåœæ­¢æœåŠ¡")
            
            # ä¿æŒæœåŠ¡è¿è¡Œ
            try:
                process.wait()
            except KeyboardInterrupt:
                print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡...")
                process.terminate()
                process.wait()
                print("âœ… æœåŠ¡å·²å…³é—­")
        else:
            print("âŒ æœåŠ¡å¯åŠ¨å¤±è´¥")
            process.terminate()
            return False
            
    except Exception as e:
        print(f"âŒ å¯åŠ¨æœåŠ¡æ—¶å‡ºé”™: {e}")
        return False
    
    return True

def check_service_health(port):
    """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
    try:
        response = requests.get(f"http://localhost:{port}/health", timeout=5)
        return response.status_code == 200
    except:
        return False

def test_openai_compatibility(port):
    """æµ‹è¯•OpenAIå…¼å®¹æ€§"""
    try:
        # æµ‹è¯•èŠå¤©å®ŒæˆAPI
        url = f"http://localhost:{port}/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "microsoft/DialoGPT-medium",
            "messages": [{"role": "user", "content": "Hello!"}],
            "max_tokens": 50
        }
        
        response = requests.post(url, headers=headers, json=data, timeout=10)
        if response.status_code == 200:
            print("âœ… OpenAIå…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
            return True
        else:
            print(f"âŒ OpenAIå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ OpenAIå…¼å®¹æ€§æµ‹è¯•å‡ºé”™: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ vLLMæœ¬åœ°æœåŠ¡å¯åŠ¨å™¨")
    print("=" * 50)
    
    # æ£€æŸ¥vLLMå®‰è£…
    if not check_vllm_installation():
        sys.exit(1)
    
    # é…ç½®å‚æ•°
    model_name = "microsoft/DialoGPT-medium"  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
    port = 8000
    
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   ç«¯å£: {port}")
    print(f"   ç”¨é€”: æœ¬åœ°æµ‹è¯•VerilogCoderé¡¹ç›®")
    
    # å¯åŠ¨æœåŠ¡
    if start_vllm_service(model_name, port):
        print("\nğŸ‰ æœåŠ¡å¯åŠ¨å®Œæˆï¼")
    else:
        print("\nâŒ æœåŠ¡å¯åŠ¨å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()
